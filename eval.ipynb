{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651bd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "print(torch.__version__)\n",
    "\n",
    "# qwen2-7B: Qwen/Qwen2-7B\n",
    "# qwen2-7B-Instruct: Qwen/Qwen2-7B-Instruct\n",
    "# qwen3-4B: Qwen/Qwen3-4B\n",
    "# Llama: meta-llama/Meta-Llama-3-8B\n",
    "# baichuan: baichuan-inc/Baichuan2-13B\n",
    "# yi: 01ai/Yi-1.5-6B\n",
    "# yi: 01ai/Yi-6B\n",
    "\n",
    "model_path = 'Qwen/Qwen3-4B'\n",
    "model_name = 'Qwen3-4B'\n",
    "\n",
    "# model_path = '01ai/Yi-1.5-34B-Chat'\n",
    "# model_name = 'Yi-1.5-34B-Chat'\n",
    "\n",
    "# model_path = 'baichuan-inc/Baichuan2-13B-Base'\n",
    "# model_name = 'Baichuan2-13B'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side='left')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Model loaded in {end_time - start_time:.2f} seconds\")\n",
    "print(f'Model name is {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4557689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.system('nvidia-smi')\n",
    "def memory_display():\n",
    "    output = subprocess.check_output(\"nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits\", shell=True)\n",
    "    memory_usage = output.decode('utf-8').strip().split('\\n')\n",
    "    total_memory = sum(int(mem) for mem in memory_usage)/1024\n",
    "    print(f\"Total GPU Memory Used: {total_memory: .2f} GB\") \n",
    "    return\n",
    "\n",
    "memory_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02ca640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_into_batches(lst, batch_size):\n",
    "    \"\"\"split data in batch\"\"\"\n",
    "    return [lst[i:i + batch_size] for i in range(0, len(lst), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24627fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Evaluation\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# few-shot\n",
    "few_shot = True\n",
    "# few_shot = ''\n",
    "\n",
    "fs_prompt = ''\n",
    "# fs_prompt = '模仿#示例1#和#示例2#，给出#示例3#的答案。'\n",
    "fs_1 = \"*句子*：快速加热（功能）对于___（品类）很重要\\n*选项*：['微波炉', '手电筒', '电风扇', '音响']\\n*答案*：微波炉\"\n",
    "fs_2 = \"*句子*：保暖（功能）对于___（品类）很重要\\n*选项*：['短袖衫', '羽绒服', '拖鞋', '太阳帽']\\n*答案*：羽绒服\"\n",
    "suffix = \"选择*选项*中相对来说最合适的一个作为*答案*填入空格处来补全以下*句子*，使其符合电子商务和生活消费领域中的逻辑和知识，输出选择的*选项*即可。括号内是词语的标签。\"\n",
    "\n",
    "# if 'Qwen' in model_name:\n",
    "#     new_eos_token_id = 151645 # <|im_end|>\n",
    "# #     new_eos_token_id = 151643 # <|endoftext|>\n",
    "#     tokenizer.eos_token_id = new_eos_token_id\n",
    "\n",
    "input_path = './Datasets/ECKGBench.jsonl'\n",
    "\n",
    "system_prompt = '直接输出答案。'\n",
    "\n",
    "\n",
    "if few_shot:\n",
    "    output_path = f'./outputs/{model_name}_fs'\n",
    "else:\n",
    "    output_path = f'./outputs/{model_name}'\n",
    "\n",
    "# print(output_path)\n",
    "f = open(input_path)\n",
    "f_out = open(output_path, 'w')\n",
    "\n",
    "total_time = 0\n",
    "batch_size = 100\n",
    "batches = split_list_into_batches(f.readlines(), batch_size)\n",
    "acc_1, acc_2, num_1, num_2 = 0,0,0,0\n",
    "for i, batch in enumerate(batches):\n",
    "    prompts = []\n",
    "    ids = []\n",
    "    gts = []\n",
    "    dims = []\n",
    "    choices = []\n",
    "    print('-'*100)\n",
    "    print('Num batch %s'%i)\n",
    "    # record batch start time\n",
    "    start_time = time.time()\n",
    "    for line_i, line in enumerate(batch):\n",
    "        cur_dict = json.loads(line)\n",
    "        uniq_id = cur_dict['id']                      \n",
    "        question, gt, dim = cur_dict['question'],cur_dict['gt'],cur_dict['dim']\n",
    "        choice = question.split('*选项*：')[1]\n",
    "        \n",
    "        if few_shot == True:\n",
    "            _, example = cur_dict['question'].split('\\n*句子*：') \n",
    "            question  = suffix +fs_prompt +'\\n#示例1#\\n'+fs_1+'\\n#示例2#\\n'+fs_2 +'\\n#示例3#\\n*句子*：'+example+'\\n*答案*：'   \n",
    "\n",
    "        if 'instruct' or 'chat' in model_name.lower():\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            question = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "        ids.append(uniq_id)\n",
    "        prompts.append(question)\n",
    "        gts.append(gt)\n",
    "        dims.append(dim)  \n",
    "        choices.append(choice)\n",
    "        \n",
    "    # model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    model_inputs = tokenizer(prompts, return_tensors=\"pt\",padding=True).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=8,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    batch_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "#     print(batch_response)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for uniq_id, response, gt, dim, choice in zip(ids,batch_response,gts,dims,choices):\n",
    "        response = response.split('\\n\\n')[0] if '\\n\\n' in response else response\n",
    "        correct = gt in response and choice not in response\n",
    "#         correct = gt in response\n",
    "        if choice in response: \n",
    "            print(response)\n",
    "            print(correct)\n",
    "\n",
    "        if dim == 'dim_1':\n",
    "            acc_1 += correct\n",
    "            num_1 += 1\n",
    "        else:\n",
    "            acc_2 += correct\n",
    "            num_2 += 1\n",
    "        output_info = {\"uniq_id\": uniq_id, \"content\": response,\"gt\":gt}\n",
    "        json.dump(output_info, f_out, ensure_ascii=False)\n",
    "        f_out.write('\\n')\n",
    "        \n",
    "    # record batch finish time     \n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time\n",
    "    print(f\"Batch finished in {end_time - start_time:.2f} seconds\")\n",
    "    torch.cuda.empty_cache()    \n",
    "\n",
    "print(f\"finished in {total_time:.2f} seconds\")\n",
    "memory_display()\n",
    "print('num_1 = ', num_1)\n",
    "print('acc_1 = ', round(100*acc_1/num_1,2))\n",
    "print('num_2 = ', num_2)\n",
    "print('acc_2 = ', round(100*acc_2/num_2,2))\n",
    "print('num = ', num_1+num_2)\n",
    "print('acc = ', round(100*(acc_1+acc_2)/(num_1+num_2),2))\n",
    "f_out.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c1050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowlegde Boundary\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# # few-shot\n",
    "# fs_prompt = '模仿#示例1#和#示例2#，给出#示例3#的答案。'\n",
    "# fs_1 = \"*句子*：快速加热（功能）对于___（品类）很重要\\n*选项*：['微波炉', '手电筒', '电风扇', '音响']\\n*答案*：微波炉\"\n",
    "# fs_2 = \"*句子*：保暖（功能）对于___（品类）很重要\\n*选项*：['短袖衫', '羽绒服', '拖鞋', '太阳帽']\\n*答案*：羽绒服\"\n",
    "# suffix = \"选择*选项*中相对来说最合适的一个作为*答案*填入空格处来补全以下*句子*，使其符合电子商务和生活消费领域中的逻辑和知识，输出选择的*选项*即可。括号内是词语的标签。\"\n",
    "\n",
    "# if 'Qwen' in model_name:\n",
    "#     new_eos_token_id = 151645 # <|im_end|>\n",
    "# #     new_eos_token_id = 151643 # <|endoftext|>\n",
    "#     tokenizer.eos_token_id = new_eos_token_id\n",
    "\n",
    "input_path = './Datasets/ECKGBench.jsonl'\n",
    "system_prompt = '直接输出答案。'\n",
    "\n",
    "output_path = f'./outputs/{model_name}_sample'\n",
    "\n",
    "# print(output_path)\n",
    "f = open(input_path)\n",
    "f_out = open(output_path, 'w')\n",
    "\n",
    "width = 200\n",
    "num_return_sequences = 5\n",
    "batch_size = int(width/num_return_sequences)\n",
    "\n",
    "batches = split_list_into_batches(f.readlines(), batch_size)\n",
    "\n",
    "hit_1, hit_2, recall_1, recall_2, corr_1, corr_2, num_1, num_2 = 0,0,0,0,0,0,0,0\n",
    "highlyknown, maybeknown, unknown = 0,0,0\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    prompts = []\n",
    "    ids = []\n",
    "    gts = []\n",
    "    dims = []\n",
    "    choices = []\n",
    "    print('-'*100)\n",
    "    print('Num batch %s'%i)\n",
    "    # record batch start time\n",
    "    start_time = time.time()\n",
    "    for line_i, line in enumerate(batch):\n",
    "        cur_dict = json.loads(line)\n",
    "        uniq_id = cur_dict['id']                      \n",
    "        question, gt, dim = cur_dict['question'],cur_dict['gt'],cur_dict['dim']\n",
    "        choice = question.split('*选项*：')[1]\n",
    "         \n",
    "        ids.append(uniq_id)\n",
    "        prompts.append(question)\n",
    "        gts.append(gt)\n",
    "        dims.append(dim)  \n",
    "        choices.append(choice)\n",
    "        \n",
    "    model_inputs = tokenizer(prompts, return_tensors=\"pt\",padding=True).to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids, \n",
    "        max_new_tokens=8, \n",
    "#         repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,   # 设置 EOS Token ID\n",
    "        do_sample=True,  # 启用采样方法\n",
    "#         top_p=0.95,      # 设置 top-p 参数\n",
    "        temperature = 0.2,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "    )\n",
    "#     print(len(generated_ids))\n",
    "    generated_ids_batch = split_list_into_batches(generated_ids, num_return_sequences)\n",
    "    batch_response = []\n",
    "    for input_ids, generated_ids in zip(model_inputs.input_ids,generated_ids_batch):\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for output_ids in generated_ids\n",
    "        ]\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        batch_response.append(response)\n",
    "#     print(batch_response)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for uniq_id, response, gt, dim, choice in zip(ids,batch_response,gts,dims,choices):\n",
    "        # hit rate\n",
    "        hit = sum(gt in re and choice not in re for re in response)/len(response)\n",
    "        # recall\n",
    "        recall = any(gt in re and choice not in re for re in response)\n",
    "        # strict correct\n",
    "        corr = all(gt in re and choice not in re for re in response)\n",
    "        \n",
    "        if corr: highlyknown += 1\n",
    "        elif (not corr) and recall: maybeknown += 1\n",
    "        elif not recall: unknown += 1\n",
    "\n",
    "        if dim == 'dim_1':\n",
    "            hit_1 += hit\n",
    "            recall_1 += recall\n",
    "            corr_1 += corr\n",
    "            num_1 += 1\n",
    "        else:\n",
    "            hit_2 += hit\n",
    "            recall_2 += recall\n",
    "            corr_2 += corr\n",
    "            num_2 += 1\n",
    "        output_info = {\"uniq_id\": uniq_id, \"content\": response,\"gt\":gt}\n",
    "        json.dump(output_info, f_out, ensure_ascii=False)\n",
    "        f_out.write('\\n')\n",
    "        \n",
    "    # record batch finish time\n",
    "    end_time = time.time()\n",
    "    print(f\"Batch finished in {end_time - start_time:.2f} seconds\")\n",
    "    torch.cuda.empty_cache()    \n",
    "\n",
    "print('HR_1 = ', round(100*hit_1/num_1,2))\n",
    "print('HR_2 = ', round(100*hit_2/num_2,2))\n",
    "print('HR = ', round(100*(hit_1+hit_2)/(num_1+num_2),2))\n",
    "\n",
    "print('Recall_1 = ', round(100*recall_1/num_1,2))\n",
    "print('Recall_2 = ', round(100*recall_2/num_2,2))\n",
    "print('Recall = ', round(100*(recall_1+recall_2)/(num_1+num_2),2))\n",
    "\n",
    "print('Corr_1 = ', round(100*corr_1/num_1,2))\n",
    "print('Corr_2 = ', round(100*corr_2/num_2,2))\n",
    "print('Corr = ', round(100*(corr_1+corr_2)/(num_1+num_2),2))\n",
    "\n",
    "\n",
    "print('know_level_1 = ', round(100*highlyknown/(num_1+num_2),2))\n",
    "print('know_level_2 = ', round(100*maybeknown/(num_1+num_2),2))\n",
    "print('know_level_3 = ', round(100*(unknown)/(num_1+num_2),2))\n",
    "\n",
    "f_out.close()\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
